<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="Coursera_practical_ML_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Coursera_practical_ML_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Coursera_practical_ML_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Coursera_practical_ML_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Coursera_practical_ML_files/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="Coursera_practical_ML_files/highlight/default.css"
      type="text/css" />
<script src="Coursera_practical_ML_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="Coursera_practical_ML_files/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">




</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Human Activity Recognition- <strong>HAR</strong> includes tracking and recognizing from simple human activities like walking, running to more complex activities like cooking,cleaning, etc. in a real life setting. Recognizing and monitoring human activities is very crucial to provide smarter and effective assistance in different fields of life. As an example, say providing health care assistance to elderly people, physically/mentally disable people, or even children.</p>
<p>With the invention of smartphones and wearable devices like Jawbone Up, Nike FuelBand, and Fitbit collecting HAR data is much simpler, effective and inexpensive but the big challenge is how to effective utilize the data, how to find a pattern in the measurement taken from these type of devices to improve the health and fitness of the user. Most of the research on activity recognition mainly focuses on predicting “type of activity” rather than predicting the quality or “how well the activity is performed”. In this project, my goal will be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants and create a model that can predict whether the barbell lifts is correct or incorrect. This project is inspired by the paper <a href="http://groupware.les.inf.puc-rio.br/har">Qualitative Activity Recognition of Weight Lifting Exercises</a> by <em>Velloso, Gellersen,Ugulino,Bulling,Fulks</em></p>
<div id="data-description" class="section level2">
<h2>Data Description</h2>
<p>Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to specification(classe A), throwing the elbows to the front(Classe B), lifting the dumbbell only half(classe C),lowering the dumbbell only halfway(classe D) and throwing the hips to front( Classe E). Classe A is correct way of doing the lifting exercise and rest all are incorrect way of doing it. Age of participant aged between <strong>20-28</strong> years and weight of dumbbell <strong>1.25kg</strong>.</p>
</div>
<div id="data-cleansing-and-processing." class="section level2">
<h2>Data Cleansing and Processing.</h2>
<p>The training and testing data set is download from the csv files <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">pml-testing.csv</a> and <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">pml-training.csv</a> into Train_data and Test_data</p>
<p>The Train_data dataset has 19622 observation and 160 variables while the Test_data has 20 observations</p>
<p>The dataset has 160 variables, all the variables may not necessarily influence the prediction values <strong>classe</strong>. Before working on model development process, lets trim and cleanse the data .</p>
<p>It is a good idea to drop all the variables that does not effect the outcome variable like user_name,timestamps, X (indicates the row number). Column 1 to 7 is removed.</p>
<pre><code>## [1] 19622   153</code></pre>
<p><strong>Zero and near-zero variance predictors</strong><br />
Constant and almost constant predictors across samples are called zero and near-zero variance predictors. This kind of predictor is not only non-informative, it can break some models you may want to fit to your data. Lets remove all the zero and near-zero variance using the</p>
<pre class="r"><code>library(randomForest)
library(caret)
nzv1 &lt;- nearZeroVar(wl_data)
filter_Wl_data &lt;- wl_data[, -nzv1]
dim(filter_Wl_data)</code></pre>
<pre><code>## [1] 19622    94</code></pre>
<p><strong>Removing the NAs</strong></p>
<p>The NA or missing values may give errors during the training of models, if it is not handled properly. Lets remove all the columns where more 90% of the data is NA, using the describe() function in psych package</p>
<pre class="r"><code>library(psych)
d &lt;- describe(filter_Wl_data)
p &lt;- d[d$n/dim(filter_Wl_data)[1] &lt; 0.1,1]
final_data &lt;- filter_Wl_data[,-p]
dim(final_data)</code></pre>
<pre><code>## [1] 19622    53</code></pre>
<p>Partitioning the data into two set training test and testing test to train modelas and validate it. 75% of data as training data and remaining 25% as testing data.</p>
<pre class="r"><code>library(caret)
set.seed(1117)
dtrain &lt;- createDataPartition(final_data$classe,
                              p=0.75, list = FALSE)
training &lt;- final_data[dtrain,]
testing &lt;- final_data[-dtrain,]
dim(training); dim(testing)</code></pre>
<pre><code>## [1] 14718    53</code></pre>
<pre><code>## [1] 4904   53</code></pre>
<div id="data-visualization" class="section level3">
<h3>Data Visualization</h3>
<p>It important here to visualized the data and see if we can see some concrete patterns that distinguise one class from the other classes.</p>
<p>I wanted to see the average roll_belt data for each weight lifting class. So I plotted the roll_belt versus their index.</p>
<p><img src="Coursera_practical_ML_files/figure-html/unnamed-chunk-7-1.png" alt="" /><!-- --></p>
<p>From the above plot, the only thing that can be infered is that, if the roll_belt measurement is more than 130 or if it is negative value the weight lifting classes is most likely incorrect (classe E or Classe E and D respectively).</p>
<p>Lets plot some more variables, to get some more insight about the data</p>
<p><img src="Coursera_practical_ML_files/figure-html/unnamed-chunk-8-1.png" alt="" /><!-- --></p>
<p>Definitely, we see patterns when we plot variable <em>yaw_belt</em> versus variable <em>pitch_forearm</em>. Lets explore different classification algorithms to see which are important variables and which is the best model.</p>
</div>
</div>
<div id="prediction-algorithms" class="section level2">
<h2>Prediction Algorithms</h2>
<p>In this project, I have tried four different classification models : Decision trees,Linear discriminant analysis,Generalized Boosted Regression Modeling and Random forest. I computed the accuracy for each of these models. The best classification model for this probelm was selected the predicting the Test_Data set.</p>
<div id="decision-tree" class="section level3">
<h3>Decision Tree</h3>
<p>Lets start with a Decision tree model forcalssification. It is one most common and simplest classification algorithm. I use the rpart() functio to create the decision tree.</p>
<p><img src="Coursera_practical_ML_files/figure-html/unnamed-chunk-9-1.png" alt="" /><!-- --></p>
<pre><code>##  Accuracy 
## 0.7373573</code></pre>
<p>The accuracy is 0.7373573.</p>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3">
<h3>Linear discriminant analysis (LDA)</h3>
<p>LDA is a Classification algorithm to find a linear combination of features that characterizes or seperates two or more classes of objects or events. It is best to use when classification has more than 2 classes.</p>
<pre><code>##  Accuracy 
## 0.7037113</code></pre>
</div>
<div id="generalized-boosted-regression-modeling-gbm" class="section level3">
<h3>Generalized Boosted Regression Modeling (gbm)</h3>
<p>It fits generalized boosted regression models.</p>
<pre><code>##  Accuracy 
## 0.4967374</code></pre>
</div>
<div id="random-tree" class="section level3">
<h3>Random Tree</h3>
<p>Random Forest is an extension of the decision tree algorithm. The core Idea behind Ranodm Forest is to generate multiple small decision trees from random subsets of data.</p>
<pre><code>## 
## Call:
##  randomForest(formula = classe ~ ., data = training) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 0.47%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 4182    2    0    0    1 0.0007168459
## B   12 2832    4    0    0 0.0056179775
## C    0   14 2548    5    0 0.0074016362
## D    0    0   25 2386    1 0.0107794362
## E    0    0    1    4 2701 0.0018477458</code></pre>
<p>Number of trees used in the fores is 500, which is default. The number of predictive variables considered at each split within a tree is 7.</p>
<p>The random forest iteratively uses a different subset of the data to make multiple decision trees. At each iteration, the tree created using the subset is tested with the data that is not used to create the tree. The average of errors of all these interactions is the Out of Bag Error (<strong>OOB</strong>). For this model OOB error is 0.46%</p>
<p>Lets use the plot() function in random Forest. This plot helps decide how many trees to have in the model. On the y-axis is the error of the model and the x-axis is the number of trees used.</p>
<p><img src="Coursera_practical_ML_files/figure-html/unnamed-chunk-13-1.png" alt="" /><!-- --></p>
<p>The red, green, blue, aqua, pink curves is for classe A,B,C, D,E respectively while the black curve is the Out-of-Bag error rate. when using between 0 - 20 trees the error remains quite high, but drops and flattens out at around 60 trees. There is no additional drop for any classes after 100 trees, therefore no need to include additional trees to the model.</p>
<p>The variable importance plot gives the importance of each variable when classifying the data. The mean decrease gini is a measure of how each variable contributes to the purity on each node in a tree.</p>
<p><img src="Coursera_practical_ML_files/figure-html/unnamed-chunk-14-1.png" alt="" /><!-- --></p>
<p>From the plot above, the most important variable as per the random forest model above is roll_belt.</p>
<p>Lets predict the testing data and compute the accuracy.</p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1395    3    0    0    0
##          B    0  944    5    0    0
##          C    0    2  849    3    0
##          D    0    0    1  800    3
##          E    0    0    0    1  898
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9963          
##                  95% CI : (0.9942, 0.9978)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9954          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9947   0.9930   0.9950   0.9967
## Specificity            0.9991   0.9987   0.9988   0.9990   0.9998
## Pos Pred Value         0.9979   0.9947   0.9941   0.9950   0.9989
## Neg Pred Value         1.0000   0.9987   0.9985   0.9990   0.9993
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2845   0.1925   0.1731   0.1631   0.1831
## Detection Prevalence   0.2851   0.1935   0.1741   0.1639   0.1833
## Balanced Accuracy      0.9996   0.9967   0.9959   0.9970   0.9982</code></pre>
<p>The model accuracy is 0.99 , which is more than any other model</p>
</div>
<div id="final-model" class="section level3">
<h3>Final Model</h3>
<p>As the random forest model gives the best prediction, I will use this model to predict the classification for Test_data.</p>
<p><strong>Predicting the data set</strong></p>
<pre class="r"><code>pred_test_data &lt;- predict(rf_mdl, Test_data)

# reading the predicted value in csv file

c &lt;- cbind(Test_data,pred_test_data)
New_Test_data &lt;- c[,-160]
write.csv(New_Test_data, file = &quot;new-pml-test.csv&quot;)</code></pre>
<p>The predicted outcome for the Test_data set using the RandomForest Model is <strong>B, A, B, A, A, E, D, B, A, A, B, C, B, A, E, E, A, B, B, B</strong></p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
